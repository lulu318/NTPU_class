# -*- coding: utf-8 -*-
"""
crawler_pipeline.py
æ•´åˆæµç¨‹ï¼š
1ï¸âƒ£ å¾ ntpu_paper.sqlite çš„ links è®€å–ç¶²å€
2ï¸âƒ£ ç”¨ Selenium çˆ¬å–æ–‡ç« æ¨™é¡Œã€æ™‚é–“ã€å…§å®¹èˆ‡ç•™è¨€
3ï¸âƒ£ å¯«å…¥åŒä¸€å€‹ DB çš„ articles è¡¨ï¼Œä¸¦ä»¥ link_id åšé—œè¯
"""

import sqlite3
import time
from datetime import datetime
from selenium import webdriver
from bson import ObjectId
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# === è³‡æ–™åº«è¨­å®š ===
DB_FILE = "/Users/lulutsai/Documents/NTPU_class/paper/code/mobile01_clawler/ntpu_paper.sqlite"

# === æ§åˆ¶ä»Šæ—¥çˆ¬å–ç­†æ•¸ ===
LIMIT_COUNT = 50  # â† æƒ³çˆ¬å¹¾ç­†å°±æ”¹é€™è£¡

# === å»ºç«‹è³‡æ–™è¡¨ï¼ˆè‹¥ä¸å­˜åœ¨ï¼‰ ===
def init_db():
    conn = sqlite3.connect(DB_FILE)
    cur = conn.cursor()

    # å»ºç«‹ articles è¡¨
    cur.execute("""
    CREATE TABLE IF NOT EXISTS articles (
        id TEXT PRIMARY KEY, 
        link_id TEXT NOT NULL,
        url TEXT,
        title TEXT,
        post_time TEXT,
        content TEXT,
        word_count INTEGER,
        replies TEXT,
        reply_count INTEGER,        -- æ–°å¢ç•™è¨€æ•¸æ¬„ä½
        created_at TEXT,
        FOREIGN KEY (link_id) REFERENCES links(id)
    )
    """)

    # æª¢æŸ¥ links è¡¨æ˜¯å¦æœ‰ status æ¬„ä½
    cur.execute("PRAGMA table_info(links);")
    columns = [c[1] for c in cur.fetchall()]
    if "status" not in columns:
        print("ğŸ§± æ–°å¢ links.status æ¬„ä½...")
        cur.execute("ALTER TABLE links ADD COLUMN status TEXT DEFAULT 'pending';")

    conn.commit()
    conn.close()

# === åˆå§‹åŒ– Chrome è¨­å®š ===
def get_driver():
    options = Options()
    # options.add_argument("--headless")  # è‹¥è¦çœ‹åˆ°ç€è¦½å™¨å¯è¨»è§£æ‰é€™è¡Œ
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    return webdriver.Chrome(options=options)

# === ä¸»ç¨‹å¼ ===
def crawl_and_store():
    conn = sqlite3.connect(DB_FILE)
    cur = conn.cursor()

    # åªå–ã€Œé‚„æ²’çˆ¬éã€çš„ links
    cur.execute("""
        SELECT id, url FROM links
        WHERE status = 'pending'
        LIMIT ?;
    """, (LIMIT_COUNT,))
    links = cur.fetchall()

    print(f"ğŸ“„ ä»Šæ—¥è¨­å®šçˆ¬å– {LIMIT_COUNT} ç­†ï¼Œå¯¦éš›å¯çˆ¬ {len(links)} ç­†ç¶²å€ã€‚")

    for i, (link_id, url) in enumerate(links, start=1):
        driver = get_driver()
        try:
            print(f"\nğŸ”— æ­£åœ¨çˆ¬å– link_id={link_id} URL={url}")
            driver.get(url)

            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "h1.t2"))
            )

            # æŠ“æ¨™é¡Œ
            title = driver.find_element(By.CSS_SELECTOR, "h1.t2").text.strip()

            # æŠ“ç™¼æ–‡æ™‚é–“
            try:
                post_time = driver.find_element(By.CSS_SELECTOR, ".o-fNotes.o-fSubMini").text.strip()
            except:
                post_time = "[ç„¡æ™‚é–“]"

            # è‹¥æ˜¯ç¬¬2é ä»¥å¾Œï¼ŒåªæŠ“ç•™è¨€ä¸æŠ“ä¸»æ–‡
            try:
                content = driver.find_element(By.CSS_SELECTOR, "div[itemprop='articleBody']").text.strip()
            except:
                print(f"æ‰¾ä¸åˆ°ä¸»æ–‡å…§å®¹ link_id={link_id}")
                content = "[ç„¡ä¸»æ–‡]"

            # æŠ“ç•™è¨€
            all_articles = driver.find_elements(By.CSS_SELECTOR, "article.c-articleLimit")
            replies = []
            for a in all_articles[1:]:  # è·³éä¸»æ–‡
                reply = a.text.strip()
                if reply.startswith("è‡ª") and "å¼•ç”¨" in reply:
                    continue
                replies.append(reply)
            replies_joined = "\n\n---\n\n".join(replies)

            # å¯«å…¥è³‡æ–™
            conn.execute("""
                INSERT INTO articles (id, link_id, url, title, post_time, content, word_count, replies, reply_count, created_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                str(ObjectId()),
                link_id,
                url,
                title,
                post_time,
                content,
                len(content),        # â† è‡ªå‹•å¡«å­—æ•¸
                replies_joined,
                len(replies),
                datetime.now().isoformat()
            ))
            conn.commit()

             # æˆåŠŸ â†’ æ¨™è¨˜å·²å®Œæˆ
            cur.execute("UPDATE links SET status='done' WHERE id=?", (link_id,))
            conn.commit()

            print(f"âœ… æˆåŠŸå¯«å…¥ link_id={link_id}ï¼ˆç•™è¨€æ•¸ï¼š{len(replies)}ï¼‰")
            time.sleep(10)

        except Exception as e:
            print(f"âš ï¸ è§£æå¤±æ•— link_id={link_id}ï¼š{e}")
            # å¤±æ•— â†’ æ¨™è¨˜ errorï¼Œé¿å…é‡è¤‡å¡æ­»
            cur.execute("UPDATE links SET status='error' WHERE id=?", (link_id,))
            conn.commit()
            time.sleep(10)
        finally:
            driver.quit()

        # ğŸ’¤ æ¯ 10 ç­†ä¼‘æ¯ä¸€æ¬¡ï¼ˆæ”¾åœ¨é€™è£¡æ¯”è¼ƒåˆç†ï¼‰
        if i % 10 == 0:
            print("ğŸ˜´ é€£çºŒçˆ¬äº† 10 ç­†ï¼Œä¼‘æ¯ 60 ç§’ä¸­â€¦")
            time.sleep(60)

    conn.close()
    print("\nğŸ‰ ä»Šæ—¥ä»»å‹™å®Œæˆï¼")


if __name__ == "__main__":
    init_db()
    crawl_and_store()
