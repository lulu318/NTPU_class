from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time
import sqlite3
from datetime import datetime
from bson import ObjectId
from datetime import datetime

# === è³‡æ–™åº«è¨­å®š ===
DB_FILE = "/Users/lulutsai/Documents/NTPU_class/paper/code/mobile01_clawler/ntpu_paper.sqlite"

def get_conn():
    conn = sqlite3.connect(DB_FILE)
    conn.execute("PRAGMA foreign_keys = ON;")
    return conn

def create_table_links(conn):
    conn.execute("""
        CREATE TABLE IF NOT EXISTS links (
            id TEXT PRIMARY KEY,          -- MongoDB ObjectId å­—ä¸²
            url TEXT,
            title TEXT,
            description TEXT,
            crawl_time TEXT,              -- ISO æ ¼å¼å­—ä¸²
            keyword TEXT,                 -- æœå°‹é—œéµå­—
            has_emotional_abuse INTEGER   -- æ˜¯å¦å«æœ‰ã€Œæƒ…ç·’å‹’ç´¢ã€ï¼š1 æˆ– 0
        );
    """)
    conn.execute("CREATE INDEX IF NOT EXISTS idx_links_url ON links(url);")
    conn.execute("CREATE INDEX IF NOT EXISTS idx_links_crawl_time ON links(crawl_time);")
    conn.execute("CREATE INDEX IF NOT EXISTS idx_links_keyword ON links(keyword);")


def insert_new_links(conn, id=None, url=None, title=None, description=None, crawl_time=None, keyword=None, has_emotional_abuse=None):
    id = id or str(ObjectId())
    crawl_time = crawl_time or datetime.now().isoformat()
    conn.execute("""
        INSERT OR IGNORE INTO links (id, url, title, description, crawl_time, keyword, has_emotional_abuse)
        VALUES (?, ?, ?, ?, ?, ?, ?);
    """, (id, url, title, description, crawl_time, keyword, has_emotional_abuse))



def get_existing_urls(conn):
    cursor = conn.execute("SELECT url FROM links;")
    return set(row[0] for row in cursor.fetchall())

def jump_to_page(driver, page_number):
    WebDriverWait(driver, 10).until(
        EC.presence_of_all_elements_located((By.CSS_SELECTOR, ".gsc-cursor-page"))
    )
    page_buttons = driver.find_elements(By.CSS_SELECTOR, ".gsc-cursor-page")
    for btn in page_buttons:
        if btn.text.strip() == str(page_number):
            driver.execute_script("arguments[0].scrollIntoView(true);", btn)
            WebDriverWait(driver, 5).until(
                EC.element_to_be_clickable((By.XPATH, f"//div[text()='{page_number}']"))
            )
            btn.click()
            print(f"ğŸš€ å·²è·³è½‰è‡³ç¬¬ {page_number} é ")
            time.sleep(2)
            return True
    print(f"âŒ æ‰¾ä¸åˆ°ç¬¬ {page_number} é çš„æŒ‰éˆ•")
    return False


# === æŠ“å– Mobile01 æœå°‹çµæœ ===
def scrape_mobile01(keyword, max_pages=5, start_page=1):
    from selenium import webdriver
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.webdriver.chrome.options import Options
    import time

    options = Options()
    options.add_argument('--disable-blink-features=AutomationControlled')
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/120 Safari/537.36")
    driver = webdriver.Chrome(options=options)

    url = f"https://www.mobile01.com/googlesearch.php?query={keyword}"
    driver.get(url)

    # âœ… è·³è½‰åˆ°èµ·å§‹é 
    if start_page > 1:
        success = jump_to_page(driver, start_page)
        if not success:
            driver.quit()
            return []

    seen_links = set()
    unique_articles = []
    page = start_page
    pages_crawled = 0

    while True:
        print(f"ğŸ“„ æŠ“å–ç¬¬ {page} é â€¦")

        try:
            WebDriverWait(driver, 10).until(
                EC.presence_of_all_elements_located((By.CSS_SELECTOR, ".gs-title a"))
            )
            time.sleep(4)
            results = driver.find_elements(By.CSS_SELECTOR, ".gs-title a")
            for r in results:
                title = r.text.strip()
                link = r.get_attribute("href")
                if link and "topicdetail.php" in link and link not in seen_links:
                    seen_links.add(link)
                    unique_articles.append((title, link))
        except Exception as e:
            print(f"âš ï¸ æŠ“å–å¤±æ•—ï¼š{e}")
            break

        pages_crawled += 1
        if pages_crawled >= max_pages:
            print(f"ğŸš« å·²é”æœ€å¤§æŠ“å–é æ•¸ {max_pages}ï¼Œåœæ­¢ã€‚")
            break

        try:
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(1.5)

            WebDriverWait(driver, 10).until(
                EC.presence_of_all_elements_located((By.CSS_SELECTOR, ".gsc-cursor-page"))
            )
            pages = driver.find_elements(By.CSS_SELECTOR, ".gsc-cursor-page")
            current_page_elements = driver.find_elements(By.CSS_SELECTOR, ".gsc-cursor-page.gsc-cursor-current-page")

            if not current_page_elements or not pages:
                print("âš ï¸ ç„¡æ³•å–å¾—é ç¢¼è³‡è¨Šï¼Œåœæ­¢ç¿»é ã€‚")
                break

            current_page = current_page_elements[0]
            page_texts = [p.text.strip() for p in pages if p.text.strip()]
            try:
                current_index = page_texts.index(current_page.text.strip())
            except ValueError:
                print("âš ï¸ é ç¢¼ç´¢å¼•éŒ¯èª¤ï¼Œåœæ­¢ç¿»é ã€‚")
                break

            if current_index + 1 >= len(pages):
                print("ğŸš« æ²’æœ‰æ›´å¤šé é¢ï¼Œåœæ­¢ç¿»é ã€‚")
                break

            next_button = pages[current_index + 1]
            driver.execute_script("arguments[0].scrollIntoView(true);", next_button)
            WebDriverWait(driver, 5).until(
                EC.element_to_be_clickable((By.XPATH, f"//div[text()='{next_button.text}']"))
            )
            next_button.click()
            page += 1
            time.sleep(2)

        except Exception as e:
            print(f"ğŸš« æ‰¾ä¸åˆ°ä¸‹ä¸€é æŒ‰éˆ•ï¼Œåœæ­¢ç¿»é ã€‚ï¼ˆ{e}ï¼‰")
            break

    driver.quit()
    return unique_articles

keywords = [
    "æƒ…ç·’å‹’ç´¢",
    "æƒ…ç·’æ§åˆ¶",
    "æƒ…ç·’ç¶æ¶",
    "å†·æš´åŠ›",
    "PUA",
    "ç²¾ç¥æ§åˆ¶",
    "æ“æ§é—œä¿‚",
    "é“å¾·ç¶æ¶",
    "æ§åˆ¶æ…¾",
    "æœ‰æ¯’é—œä¿‚",
    "æ“æ§å‹äººæ ¼",
    "æƒ…æ„Ÿå‹’ç´¢",
    "æƒ…å‹’"
]

# === ä¸»ç¨‹å¼ ===
if __name__ == "__main__":
    keyword = keywords[12]
    has_emotional_abuse = 1 # é€™å€‹è¦è¨˜å¾—input çœ‹ä»–çš„é—œéµå­—æ˜¯å¦è·Ÿæƒ…ç·’å‹’ç´¢æœ‰é—œ
    articles = scrape_mobile01(keyword, max_pages=10)

    print(f"âœ… å…±æŠ“åˆ° {len(articles)} ç¯‡æ–‡ç« ï¼Œæº–å‚™å¯«å…¥è³‡æ–™åº«...")

    conn = get_conn()
    create_table_links(conn)
    existing_urls = get_existing_urls(conn)

    for title, link in articles:
        if link in existing_urls:
            print(f"â­ï¸ å·²å­˜åœ¨æ–¼è³‡æ–™åº«ï¼Œç•¥éï¼š{link}")
            continue

        doc_id = str(ObjectId())
        insert_new_links(conn,
                        id=doc_id,
                        url=link,
                        title=title,
                        description=f"ä¾†æºï¼šMobile01 æœå°‹é—œéµå­—ã€Œ{keyword}ã€",
                        keyword=keyword, 
                        has_emotional_abuse=has_emotional_abuse)
    conn.commit()
    conn.close()

    print(f"âœ… å·²å¯«å…¥ SQLiteï¼š{DB_FILE}")
